{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrV8taGh19AlXJH9PWZEWG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nghess/makemore/blob/main/makemore_lesson3b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fna-1y3Le3r-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import urllib3\n",
        "import random \n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import text file"
      ],
      "metadata": {
        "id": "lQrf8UsZlWFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grab text file from github\n",
        "http = urllib3.PoolManager()\n",
        "textfile = http.request('GET', 'https://raw.githubusercontent.com/nghess/makemore/master/names.txt')\n",
        "words = str(textfile.data, 'utf-8').splitlines()"
      ],
      "metadata": {
        "id": "Gas_xh2ViSy4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a peak at text file\n",
        "words[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpKY8UAijuA4",
        "outputId": "65e3ac64-1bc5-4eb1-8c70-99eb9f188675"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['emma',\n",
              " 'olivia',\n",
              " 'ava',\n",
              " 'isabella',\n",
              " 'sophia',\n",
              " 'charlotte',\n",
              " 'mia',\n",
              " 'amelia',\n",
              " 'harper',\n",
              " 'evelyn']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build character to/from string mappings\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s, i in stoi.items()}\n",
        "vocab_size = len(itos)\n",
        "block_size = 3  # context length: how many characters do we use to predict the next one?"
      ],
      "metadata": {
        "id": "7t4Z8wSqFHyB"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the dataset\n",
        "def build_dataset(words):\n",
        "\n",
        "  X, Y = [], []\n",
        "  for w in words:\n",
        "\n",
        "    context = [0] * block_size\n",
        "    for ch in w + '.':\n",
        "      ix = stoi[ch]\n",
        "      X.append(context)\n",
        "      Y.append(ix)\n",
        "    # print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
        "      context = context[1:] + [ix] #  Crop and append\n",
        "\n",
        "  X = torch.tensor(X)\n",
        "  Y = torch.tensor(Y)\n",
        "  print(X.shape, Y.shape)\n",
        "  return X, Y\n",
        "\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "n1 = int(0.8*len(words))\n",
        "n2 = int(0.9*len(words))\n",
        "\n",
        "Xtr, Ytr = build_dataset(words[:n1])\n",
        "Xdev, Ydev = build_dataset(words[n1:n2])\n",
        "Xte, Yte = build_dataset(words[n2:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23wu2I7QpWBL",
        "outputId": "3a4e9baf-99ea-4ed7-9770-12177a5d59df"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([182625, 3]) torch.Size([182625])\n",
            "torch.Size([22655, 3]) torch.Size([22655])\n",
            "torch.Size([22866, 3]) torch.Size([22866])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzVWk1VGrjlO",
        "outputId": "d557dc50-578a-493a-95f2-c23deb454129"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32033"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_emb = 50\n",
        "n_hidden = 200\n",
        "n_chars = len(stoi)\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "C = torch.randn((n_chars, n_emb), generator=g)\n",
        "W1 = torch.randn((n_emb*block_size, n_hidden), generator=g) * (5/3)/((n_emb*block_size)**0.5)  # Kaiming init\n",
        "#b1 = torch.randn(n_hidden, generator=g) * 0.01\n",
        "W2 = torch.randn((n_hidden, n_chars), generator=g) * 0.01\n",
        "b2 = torch.randn(n_chars, generator=g) * 0\n",
        "\n",
        "bngain = torch.ones((1, n_hidden))\n",
        "bnbias = torch.zeros((1, n_hidden))\n",
        "bnmean_running = torch.zeros((1, n_hidden))\n",
        "bnstd_running = torch.ones((1, n_hidden))\n",
        "\n",
        "parameters = [C, W1, W2, b2, bngain, bnbias]\n",
        "\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "id": "ShXHpZnPlxBx"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Km5iFPk8sfpf",
        "outputId": "5878690c-9162-4a48-cc47-5e86e7cda09d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "37177"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iters = 1#00000\n",
        "batch_size = 32\n",
        "lossi = []\n",
        "\n",
        "\n",
        "# Forward pass\n",
        "for i in range(iters):\n",
        "\n",
        "  # Minibatch construct\n",
        "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "  Xb, Yb = Xtr[ix], Ytr[ix]  # Batch x, y\n",
        "\n",
        "  # Forward pass\n",
        "  emb = C[Xb]  # (32, 2, 2) by indexing into the dataset \n",
        "  embcat = emb.view(emb.shape[0], -1)\n",
        "  # Linear layer\n",
        "  hpreact = embcat @ W1 #+ b1\n",
        "  # BatchNorm layer\n",
        "  bnmeani = hpreact.mean(0, keepdim=True)\n",
        "  bnstdi = hpreact.std(0, keepdim=True)\n",
        "  hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias\n",
        "  with torch.no_grad():\n",
        "    bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n",
        "    bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi\n",
        "  # Non-linearity\n",
        "  h = torch.tanh(hpreact)  # Hidden layer\n",
        "  logits = h @ W2 + b2  # Output layer\n",
        "  loss = F.cross_entropy(logits, Yb)\n",
        "\n",
        "  # Backward pass\n",
        "  for p in parameters:\n",
        "      p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # Update\n",
        "  lr = 0.1 if i < int(iters/2) else 0.01 \n",
        "  for p in parameters:\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "  # Track loss\n",
        "  if i % 10000 == 0: # print every 10000 iters\n",
        "    print(f'{i:7d}/{iters:7d}: {loss.item():.4f}')\n",
        "  lossi.append(loss.log10().item())"
      ],
      "metadata": {
        "id": "ULcrI6XVm1FB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2db7cf4-443b-4088-83f5-f8927ce58b3d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      0/      1: 3.2916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stepi = torch.linspace(0, 1, iters)\n",
        "plt.plot(stepi, lossi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "LzRVonS-s5gS",
        "outputId": "b6c79cbb-26d6-495f-b14c-71bc75fb25e6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f057d23e760>]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAAAPu0lEQVR4nO3dbYxc51nG8f8VWw4taWnSbEIaG9aFSPSFqJAhqBJF5cWtQcKuGgQRiMaFEFCxgooqYRQkisMHmlJAqJGQFQUZJEjAqNKGAsYtVFQqLZ4tbhonuNm4VFk30G3cVpSqiUxuPuxxOh7W3tndWY/3yf8nHfmc53nOzH17pcvHc2ZmU1VIktp12aQLkCStL4Nekhpn0EtS4wx6SWqcQS9Jjds86QKGXX311TU9PT3pMiRpQ5mdnf1iVU0tNXfJBf309DT9fn/SZUjShpLkc+eb86UbSWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNGynok+xMciLJXJJ9S8zvSbKQ5Fi33T40/9Ik80neP67CJUmjWfZ3xibZBNwL7ADmgaNJZqrq0aGlD1bV3vM8zN3AP6+pUknSqoxyRX8zMFdVJ6vqWeABYPeoT5DkJuBa4B9WV6IkaS1GCfrrgScHjue7sWG3JHk4yaEk2wCSXAa8D3jXhZ4gyR1J+kn6CwsLI5YuSRrFuG7GPgRMV9WNwBHgYDf+DuBvq2r+QidX1YGq6lVVb2pqakwlSZJghNfogVPAtoHjrd3Y86rq6YHD+4B7uv3XA29I8g7gCmBLkq9W1f+7oStJWh+jBP1R4IYk21kM+FuBnxlckOS6qnqqO9wFPAZQVT87sGYP0DPkJeniWjboq+pMkr3AYWATcH9VHU+yH+hX1QxwZ5JdwBngNLBnHWuWJK1AqmrSNZyj1+tVv9+fdBmStKEkma2q3lJzfjJWkhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxIwV9kp1JTiSZS7Jvifk9SRaSHOu227vxb0/yyW7seJJfHncDkqQL27zcgiSbgHuBHcA8cDTJTFU9OrT0waraOzT2FPD6qnomyRXAI925nx9H8ZKk5Y1yRX8zMFdVJ6vqWeABYPcoD15Vz1bVM93h5SM+nyRpjEYJ3uuBJweO57uxYbckeTjJoSTbzg4m2Zbk4e4x3rPU1XySO5L0k/QXFhZW2IIk6ULGdYX9EDBdVTcCR4CDZyeq6slu/DuB25JcO3xyVR2oql5V9aampsZUkiQJRgv6U8C2geOt3djzqurpgZdo7gNuGn6Q7kr+EeANqytVkrQaowT9UeCGJNuTbAFuBWYGFyS5buBwF/BYN741yYu6/SuBHwBOjKNwSdJoln3XTVWdSbIXOAxsAu6vquNJ9gP9qpoB7kyyCzgDnAb2dKe/CnhfkgIC/F5VfXod+pAknUeqatI1nKPX61W/3590GZK0oSSZrareUnO+3VGSGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEjBX2SnUlOJJlLsm+J+T1JFpIc67bbu/HXJfmXJMeTPJzkp8fdgCTpwjYvtyDJJuBeYAcwDxxNMlNVjw4tfbCq9g6NfQ14W1U9nuQVwGySw1X15THULkkawShX9DcDc1V1sqqeBR4Ado/y4FX1map6vNv/PPAFYGq1xUqSVm6UoL8eeHLgeL4bG3ZL9/LMoSTbhieT3AxsAZ5YYu6OJP0k/YWFhRFLlySNYlw3Yx8CpqvqRuAIcHBwMsl1wJ8Bb6+q54ZPrqoDVdWrqt7UlBf8kjROowT9KWDwCn1rN/a8qnq6qp7pDu8Dbjo7l+SlwAeBu6rq42srV5K0UqME/VHghiTbk2wBbgVmBhd0V+xn7QIe68a3AB8A/rSqDo2nZEnSSiz7rpuqOpNkL3AY2ATcX1XHk+wH+lU1A9yZZBdwBjgN7OlO/yngB4GXJzk7tqeqjo21C0nSeaWqJl3DOXq9XvX7/UmXIUkbSpLZquotNecnYyWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGjdS0CfZmeREkrkk+5aY35NkIcmxbrt9YO7vk3w5yd+Ms3BJ0mg2L7cgySbgXmAHMA8cTTJTVY8OLX2wqvYu8RDvBV4M/NJai5UkrdwoV/Q3A3NVdbKqngUeAHaP+gRV9WHgv1dZnyRpjUYJ+uuBJweO57uxYbckeTjJoSTbVlJEkjuS9JP0FxYWVnKqJGkZ47oZ+xAwXVU3AkeAgys5uaoOVFWvqnpTU1NjKkmSBKMF/Slg8Ap9azf2vKp6uqqe6Q7vA24aT3mSpLUaJeiPAjck2Z5kC3ArMDO4IMl1A4e7gMfGV6IkaS2WfddNVZ1Jshc4DGwC7q+q40n2A/2qmgHuTLILOAOcBvacPT/JR4HvAq5IMg/8QlUdHn8rkqSlpKomXcM5er1e9fv9SZchSRtKktmq6i015ydjJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaN1LQJ9mZ5ESSuST7lpjfk2QhybFuu31g7rYkj3fbbeMsXpK0vM3LLUiyCbgX2AHMA0eTzFTVo0NLH6yqvUPnXgX8FtADCpjtzv3SWKqXJC1rlCv6m4G5qjpZVc8CDwC7R3z8NwNHqup0F+5HgJ2rK1WStBqjBP31wJMDx/Pd2LBbkjyc5FCSbSs8V5K0TsZ1M/YhYLqqbmTxqv3gSk5OckeSfpL+wsLCmEqSJMFoQX8K2DZwvLUbe15VPV1Vz3SH9wE3jXpud/6BqupVVW9qamrU2iVJIxgl6I8CNyTZnmQLcCswM7ggyXUDh7uAx7r9w8CbklyZ5ErgTd2YJOkiWfZdN1V1JsleFgN6E3B/VR1Psh/oV9UMcGeSXcAZ4DSwpzv3dJK7WfzHAmB/VZ1ehz4kSeeRqpp0Defo9XrV7/cnXYYkbShJZquqt9Scn4yVpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjRsp6JPsTHIiyVySfRdYd0uSStLrjrck+ZMkn07yqSRvHE/ZkqRRbV5uQZJNwL3ADmAeOJpkpqoeHVr3EuBXgU8MDP8iQFV9d5JrgL9L8n1V9dy4GpAkXdgoV/Q3A3NVdbKqngUeAHYvse5u4D3A1wfGXg38I0BVfQH4MtBbS8GSpJUZJeivB54cOJ7vxp6X5HuBbVX1waFzPwXsSrI5yXbgJmDb8BMkuSNJP0l/YWFhRQ1Iki5s2ZdulpPkMuD3gT1LTN8PvAroA58DPgb87/CiqjoAHADo9Xq11pokSd8wStCf4tyr8K3d2FkvAV4LfCQJwLcCM0l2VVUfeOfZhUk+BnxmrUVLkkY3yks3R4EbkmxPsgW4FZg5O1lVX6mqq6tquqqmgY8Du6qqn+TFSb4ZIMkO4MzwTVxJ0vpa9oq+qs4k2QscBjYB91fV8ST7gX5VzVzg9GuAw0meY/F/AT+33PPNzs5+McnnRiv/knI18MVJF3GR2fMLgz1vDN9+volU+ZL4OCTpV9UL6h1F9vzCYM8bn5+MlaTGGfSS1DiDfnwOTLqACbDnFwZ73uB8jV6SGucVvSQ1zqCXpMYZ9CuQ5KokR5I83v155XnW3dateTzJbUvMzyR5ZP0rXru19Nx9YO6DSf49yfEkv3txqx/dcl/FneTyJA92859IMj0w9xvd+Ikkb76oha/BantOsiPJbPf147NJfviiF79Ka/k5d/PfluSrSd510Yoeh6pyG3ED7gH2dfv7gPcsseYq4GT355Xd/pUD828F/hx4ZNL9rHfPwIuBH+rWbAE+CvzYpHtaov5NwBPAK7s6PwW8emjNO4A/7vZvBR7s9l/drb8c2N49zqZJ97TOPX8P8Ipu/7XAqUn3s949D8wfAv4KeNek+1nJ5hX9yuwGDnb7B4G3LLHmzcCRqjpdVV8CjgA7AZJcAfwa8DvrX+rYrLrnqvpaVf0TQC1+xfUnWfyupEvNKF/FPfj3cAj4kSx+udNu4IGqeqaqPgvMdY93qVt1z1X1b1X1+W78OPCiJJdflKrXZi0/Z5K8Bfgsiz1vKAb9ylxbVU91+/8JXLvEmgt9rfPdwPuAr61bheO31p4BSPIy4CeAD69DjWu1bP2Da6rqDPAV4OUjnnspWkvPg24BPllVz6xTneO06p67i7RfB377ItQ5dmv+muLWJPkQi9/AOeyuwYOqqiQjvzc1yeuA76iqdw6/7jdp69XzwONvBv4C+KOqOrm6KnWpSfIaFn/Z0JsmXctF8G7gD6rqq90F/oZi0A+pqh8931yS/0pyXVU9leQ64AtLLDsFvHHgeCvwEeD1QC/Jf7D4935Nko9U1RuZsHXs+awDwONV9Ydrr3ZdLPdV3INr5rt/uL4FeHrEcy9Fa+mZJFuBDwBvq6on1r/csVhLz98P/GSSe4CXAc8l+XpVvX/dqx6HSd8k2Egb8F7OvTF5zxJrrmLxdbwru+2zwFVDa6bZODdj19Qzi/cj/hq4bNK9XKDHzSzeQN7ON27SvWZoza9w7k26v+z2X8O5N2NPsjFuxq6l55d169866T4uVs9Da97NBrsZO/ECNtLG4uuTHwYeBz40EGY94L6BdT/P4k25OeDtSzzORgr6VffM4hVTAY8Bx7rt9kn3dJ4+f5zFX4rzBHBXN7afxd+tAPBNLL7bYg74V+CVA+fe1Z13gkvwXUXj7hn4TeB/Bn6mx4BrJt3Pev+cBx5jwwW9X4EgSY3zXTeS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXu/wC02114fIj/wgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lossi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rc59mErydp1Y",
        "outputId": "d3459bc7-b9f5-473f-fe03-2cd04b2052b3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.517411470413208]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def split_loss(split):\n",
        "  x, y = {\n",
        "      'train': (Xtr, Ytr),\n",
        "      'val': (Xdev, Ydev),\n",
        "      'test': (Xte, Yte),\n",
        "  }[split]\n",
        "  emb = C[x]  # (32, 2, 2) by indexing into the dataset \n",
        "  embcat = emb.view(emb.shape[0], -1)\n",
        "  hpreact = embcat @ W1 #+ b1\n",
        "  hpreact = bngain * (hpreact - bnmean_running) / (bnstd_running + bnbias)\n",
        "  h = torch.tanh(hpreact)   # (N, nhidden)\n",
        "  logits = h @ W2 + b2  # (N, vocab size)\n",
        "  loss = F.cross_entropy(logits, y)\n",
        "  print(split, loss.item())\n",
        "\n",
        "split_loss('train')\n",
        "split_loss('val')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ANU3g4SwEl0",
        "outputId": "0b0ed07d-15c2-4f5e-bbe9-ccea144273cf"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train 3.3001785278320312\n",
            "val 3.300177574157715\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorchify the network\n",
        "\n",
        "class Linear:\n",
        "\n",
        "  def __init__(self, fan_in, fan_out, bias=True):\n",
        "    self.weight = torch.randn((fan_in, fan_out), generator=g) / fan_in**0.5\n",
        "    self.bias = torch.zeros(fan_out) if bias else None\n",
        "\n",
        "  def __call__(self, x):\n",
        "    self.out = x @ self.weight\n",
        "    if self.bias is not None:\n",
        "        self.out += self.bias\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.weight] + ([] if self.bias is None else [self.bias])\n",
        "\n",
        "class BatchNorm1d:\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.momentum = momentum\n",
        "    self.training = True\n",
        "    # Parameters (trained with backprop)\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "    # Buffers (trained with a running 'momentum update')\n",
        "    self.running_mean = torch.zeroes(dim)\n",
        "    self.running_var = torch.ones(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # Calculate the forward pass\n",
        "    if self.training:\n",
        "      xmean = x.mean(0, keepdim=True)  # Batch mean\n",
        "      xvar = x.var(0, keepdim=True, unbiased=True)  # Batch variance\n",
        "    else:\n",
        "      xmean = self.running_mean\n",
        "      xvar = self.running_var\n",
        "    xhat = (x- xmean) / torch.sqrt(xvar + self.eps)  # Normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    # Update the buffers\n",
        "    if self.training:\n",
        "      with torch.no_grad():\n",
        "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
        "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "class Tanh:\n",
        "  def _call__(self, x):\n",
        "    self.out = torch.tanh(x)\n",
        "    return self.out\n",
        "  def parameters(self):\n",
        "    return []"
      ],
      "metadata": {
        "id": "p5bUVWe1HZAJ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(p.nelement() for p in parameters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDsMPC3kP_A4",
        "outputId": "331b63dd-7446-4d21-fa88-57243edad93a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "37177"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_embd = 10  # Dimensions for character embedding vector\n",
        "n_hidden = 100  # Number of neurons in hidden layer\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "\n",
        "C = torch.randn((vocab_size, n_embd), generator=g)\n",
        "layers = [\n",
        "    Linear(n_embd * block_size, n_hidden, bias=False), Tanh(),\n",
        "    Linear(           n_hidden, n_hidden, bias=False), Tanh(),\n",
        "    Linear(           n_hidden, n_hidden, bias=False), Tanh(),\n",
        "    Linear(           n_hidden, n_hidden, bias=False), Tanh(),\n",
        "    Linear(           n_hidden, n_hidden, bias=False), Tanh(),\n",
        "    Linear(           n_hidden, vocab_size, bias=False)\n",
        "  ]\n",
        "\n",
        "with torch.no_grad():\n",
        "  # Last layer: make less confident\n",
        "  layers[-1].weight *= 0.1\n",
        "  # All other layers: apply gain\n",
        "  for layer in layers[:-1]:\n",
        "    if isinstance(layer, Linear):\n",
        "      layer.weight *= 5/3\n",
        "\n",
        "parameter = [C] + [p for layer in layers for p in layer.parameters()]"
      ],
      "metadata": {
        "id": "zPuwgOBRNPh3"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3gpjRCROqhZ",
        "outputId": "6043a3db-9292-426b-f76c-840c7e078926"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "37177\n"
          ]
        }
      ]
    }
  ]
}